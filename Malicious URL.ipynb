{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48d18909",
   "metadata": {},
   "source": [
    "**Import the Necessary Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f03374",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imblearn\n",
      "  Using cached imblearn-0.0-py2.py3-none-any.whl (1.9 kB)\n",
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n",
      "     -------------------------------------- 226.0/226.0 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gk\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (2.2.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\gk\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\gk\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\gk\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.21.5)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "     -------------------------------------- 298.0/298.0 kB 1.7 MB/s eta 0:00:00\n",
      "Installing collected packages: joblib, imbalanced-learn, imblearn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.0\n",
      "    Uninstalling joblib-1.1.0:\n",
      "      Successfully uninstalled joblib-1.1.0\n",
      "Successfully installed imbalanced-learn-0.10.1 imblearn-0.0 joblib-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae581c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import pickle\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42357047",
   "metadata": {},
   "source": [
    "**Load the Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40df98f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>label</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.google.com</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.youtube.com</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.facebook.com</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.baidu.com</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.wikipedia.org</td>\n",
       "      <td>benign</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         url   label  result\n",
       "0     https://www.google.com  benign       0\n",
       "1    https://www.youtube.com  benign       0\n",
       "2   https://www.facebook.com  benign       0\n",
       "3      https://www.baidu.com  benign       0\n",
       "4  https://www.wikipedia.org  benign       0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('urldata.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695fc50d",
   "metadata": {},
   "source": [
    "**Exploring the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5ec08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('urldata.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa486dc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the data (450176, 3)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of the data',df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e53b1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 450176 entries, 0 to 450175\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   url     450176 non-null  object\n",
      " 1   label   450176 non-null  object\n",
      " 2   result  450176 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 10.3+ MB\n",
      "Summary of the data: None\n"
     ]
    }
   ],
   "source": [
    "print('Summary of the data:',df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d75b935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null Values in the data:\n",
      " url       0\n",
      "label     0\n",
      "result    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Null Values in the data:\\n',df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fb9faf",
   "metadata": {},
   "source": [
    "**Removing the label column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ef7780fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.google.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.youtube.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.facebook.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.baidu.com</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.wikipedia.org</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         url  result\n",
       "0     https://www.google.com       0\n",
       "1    https://www.youtube.com       0\n",
       "2   https://www.facebook.com       0\n",
       "3      https://www.baidu.com       0\n",
       "4  https://www.wikipedia.org       0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop('label',axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5acaf441",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    345738\n",
       "1    104438\n",
       "Name: result, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['result'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff0da30",
   "metadata": {},
   "source": [
    "*Data is Imbalanced*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc929f9",
   "metadata": {},
   "source": [
    "**Tokenizing the URL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6af50bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(i):\n",
    "    x=re.split('[/-]',i)\n",
    "    if '' in x:\n",
    "        x.remove('')\n",
    "    for j in x:\n",
    "        if j.find('.')>=0:\n",
    "            y=j.split('.')\n",
    "            if 'com' in y:\n",
    "                y.remove('com')\n",
    "            x=x+y\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fb75c9",
   "metadata": {},
   "source": [
    "**Using Vectorizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132e1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf=CountVectorizer(tokenizer=tokenizer)\n",
    "tf=TfidfVectorizer(tokenizer=tokenizer)\n",
    "with open('vector','wb') as f:\n",
    "    pickle.dump(tf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5667dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf=CountVectorizer(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "739efefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf=TfidfVectorizer(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2659832",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vector','wb') as f:\n",
    "    pickle.dump(tf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f4dee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=cf.fit_transform(df['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca473cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2=tf.fit_transform(df['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3fbf87d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "         ..\n",
       "450171    1\n",
       "450172    1\n",
       "450173    1\n",
       "450174    1\n",
       "450175    1\n",
       "Name: result, Length: 450176, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y=df['result']\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84498a",
   "metadata": {},
   "source": [
    "**Since our data is imbalanced we can try oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "391fae8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm=SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4431cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_new,y1_new=sm.fit_resample(x1,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f25aeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_new,y2_new=sm.fit_resample(x2,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42ca1d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=ShuffleSplit(n_splits=10,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bd0ee6",
   "metadata": {},
   "source": [
    "## Trying different ML Algorithms to check for accuracy\n",
    "\n",
    " **Using Counter Vectorizer without oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec18b694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99755651 0.99754911 0.99763796 0.99783048 0.99763796 0.99780086\n",
      " 0.99778605 0.99771201 0.99760835 0.99787491]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(LogisticRegression(max_iter=500),x1,y,cv=cv,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8265a165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99677164 0.99662355 0.99649767 0.99656431 0.99678645 0.99659393\n",
      " 0.99649027 0.99658653 0.99653469 0.99676423]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(MultinomialNB(),x1,y,cv=cv,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54738ea2",
   "metadata": {},
   "source": [
    "**Using Counter Vectorizer with oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cbfd3fc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99690999 0.99703533 0.99685697 0.99689071 0.99678948 0.99726672\n",
      " 0.9968184  0.99701605 0.99683286 0.99699194]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(LogisticRegression(max_iter=500),x1_new,y1_new,cv=cv,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "37130126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99017561 0.9905227  0.98994423 0.99026721 0.99004546 0.99059019\n",
      " 0.99055162 0.99034434 0.99017561 0.99037326]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(MultinomialNB(),x1_new,y1_new,cv=cv,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667bca32",
   "metadata": {},
   "source": [
    "**Using Tfidf Vectorizer without oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b60f8ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98836753 0.98850822 0.98808616 0.98856745 0.98822684 0.9881602\n",
      " 0.98852302 0.98853043 0.98827127 0.98822684]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(MultinomialNB(),x2,y,cv=cv,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0eef0348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99446884 0.99428373 0.99449846 0.99443182 0.994728   0.99451327\n",
      " 0.99462433 0.99437258 0.99422449 0.99485387]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(LogisticRegression(max_iter=500),x2,y,cv=cv,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f17167",
   "metadata": {},
   "source": [
    "**Using Tfidf Vectorizer with oversampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ea0305f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99091799 0.99137594 0.99095173 0.99072998 0.99100476 0.99098548\n",
      " 0.99135666 0.99087942 0.99081675 0.99105296]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(MultinomialNB(),x2_new,y2_new,cv=cv,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cffed843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99733903 0.99736313 0.9972378  0.99730046 0.99719923 0.99744026\n",
      " 0.99738241 0.99735349 0.9971462  0.99727154]\n"
     ]
    }
   ],
   "source": [
    "print(cross_val_score(LogisticRegression(max_iter=500),x2_new,y2_new,cv=cv,scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0dd9f0",
   "metadata": {},
   "source": [
    "**Creating and Saving the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ba67e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb=MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2379816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test=train_test_split(x2_new,y2_new,test_size=0.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "bd22fb61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "03610070",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction=nb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "44af459f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9971799443426378"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb.score(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "c146191f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101922,    162],\n",
       "       [  1722, 103637]], dtype=int64)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(prediction,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "13f7d1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99    102084\n",
      "           1       1.00      0.98      0.99    105359\n",
      "\n",
      "    accuracy                           0.99    207443\n",
      "   macro avg       0.99      0.99      0.99    207443\n",
      "weighted avg       0.99      0.99      0.99    207443\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(prediction,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "791d9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('url','wb') as f:\n",
    "    pickle.dump(nb,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d285a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('vector','rb') as f:\n",
    "    vectorizer=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "032c54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(a):\n",
    "    # Initialize a new vectorizer object\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit the vectorizer object on some training data\n",
    "    vectorizer.fit(x_train)\n",
    "\n",
    "    # Transform the new data using the fitted vectorizer object\n",
    "    v = vectorizer.transform([a])\n",
    "\n",
    "    # Make a prediction using the trained model\n",
    "    p = model.predict(v)\n",
    "\n",
    "    return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1659b807",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_156\\2234638227.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'wikipedia.com'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_156\\2291075066.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m# Fit the vectorizer object on some training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m# Transform the new data using the fitted vectorizer object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2051\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2052\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_warn_for_unused_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2053\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2054\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2055\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m         \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1201\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1202\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpreprocessor\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m             \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     69\u001b[0m     \"\"\"\n\u001b[0;32m     70\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0maccent_function\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccent_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    769\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" not found\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "predict('wikipedia.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706b6d5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
